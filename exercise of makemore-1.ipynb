{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366c7283-24be-47a2-b638-a84df233a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. \n",
    "# Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd826509-e4c6-4ae9-904d-28502a07bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e6c9e56-9dc1-4bd7-b4c5-92aefddca3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "788076cc-eb49-4a47-b79f-6143d65fd24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dd9a197-ab24-4b3b-b600-16c8c863a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# creates the training set of bigrams (x, y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    # Trigram context requires TWO starting characters\n",
    "    # If word is 'emma', context is:\n",
    "    # .. -> e\n",
    "    # .e -> m\n",
    "    # em -> m\n",
    "    # mm -> a\n",
    "    # ma -> .\n",
    "    chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3] # target\n",
    "        # Create a unique idx for the input pair (ch1, ch2)\n",
    "        # this maps (0, 0) -> 0, (0, 1) -> 1 ... (26, 26) -> 728\n",
    "        trigram_idx = ix1 * 27 + ix2 # base-27\n",
    "        xs.append(trigram_idx)\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cafcb58-4064-4d42-b6b3-9aa36289f2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.8028225898742676\n",
      "loss: 3.64870285987854\n",
      "loss: 3.556884765625\n",
      "loss: 3.4891085624694824\n",
      "loss: 3.4331703186035156\n",
      "loss: 3.384026527404785\n",
      "loss: 3.3398447036743164\n",
      "loss: 3.299639940261841\n",
      "loss: 3.262803554534912\n",
      "loss: 3.2289042472839355\n",
      "loss: 3.1975934505462646\n",
      "loss: 3.1685667037963867\n",
      "loss: 3.1415536403656006\n",
      "loss: 3.1163175106048584\n",
      "loss: 3.0926547050476074\n",
      "loss: 3.0703928470611572\n",
      "loss: 3.0493879318237305\n",
      "loss: 3.0295186042785645\n",
      "loss: 3.01068377494812\n",
      "loss: 2.9927961826324463\n",
      "loss: 2.975781202316284\n",
      "loss: 2.9595749378204346\n",
      "loss: 2.944119453430176\n",
      "loss: 2.9293642044067383\n",
      "loss: 2.915264368057251\n",
      "loss: 2.901778221130371\n",
      "loss: 2.8888683319091797\n",
      "loss: 2.8765010833740234\n",
      "loss: 2.8646445274353027\n",
      "loss: 2.853269577026367\n",
      "loss: 2.842349052429199\n",
      "loss: 2.831857204437256\n",
      "loss: 2.8217711448669434\n",
      "loss: 2.812068223953247\n",
      "loss: 2.8027267456054688\n",
      "loss: 2.7937276363372803\n",
      "loss: 2.7850513458251953\n",
      "loss: 2.7766804695129395\n",
      "loss: 2.7685980796813965\n",
      "loss: 2.7607884407043457\n",
      "loss: 2.753236770629883\n",
      "loss: 2.745929718017578\n",
      "loss: 2.7388534545898438\n",
      "loss: 2.7319953441619873\n",
      "loss: 2.7253448963165283\n",
      "loss: 2.718891143798828\n",
      "loss: 2.7126240730285645\n",
      "loss: 2.7065343856811523\n",
      "loss: 2.700613021850586\n",
      "loss: 2.694852590560913\n",
      "loss: 2.6892452239990234\n",
      "loss: 2.683783531188965\n",
      "loss: 2.6784615516662598\n",
      "loss: 2.6732726097106934\n",
      "loss: 2.6682114601135254\n",
      "loss: 2.663271903991699\n",
      "loss: 2.658449411392212\n",
      "loss: 2.6537394523620605\n",
      "loss: 2.649137020111084\n",
      "loss: 2.6446382999420166\n",
      "loss: 2.6402392387390137\n",
      "loss: 2.6359357833862305\n",
      "loss: 2.6317248344421387\n",
      "loss: 2.6276028156280518\n",
      "loss: 2.6235666275024414\n",
      "loss: 2.6196131706237793\n",
      "loss: 2.6157400608062744\n",
      "loss: 2.6119437217712402\n",
      "loss: 2.608222723007202\n",
      "loss: 2.6045737266540527\n",
      "loss: 2.6009950637817383\n",
      "loss: 2.5974838733673096\n",
      "loss: 2.594038248062134\n",
      "loss: 2.5906565189361572\n",
      "loss: 2.587336540222168\n",
      "loss: 2.5840766429901123\n",
      "loss: 2.5808746814727783\n",
      "loss: 2.5777297019958496\n",
      "loss: 2.574639320373535\n",
      "loss: 2.5716025829315186\n",
      "loss: 2.568617582321167\n",
      "loss: 2.565683364868164\n",
      "loss: 2.562798261642456\n",
      "loss: 2.5599610805511475\n",
      "loss: 2.5571703910827637\n",
      "loss: 2.5544252395629883\n",
      "loss: 2.5517241954803467\n",
      "loss: 2.5490665435791016\n",
      "loss: 2.5464508533477783\n",
      "loss: 2.5438766479492188\n",
      "loss: 2.54134202003479\n",
      "loss: 2.538846492767334\n",
      "loss: 2.5363895893096924\n",
      "loss: 2.5339696407318115\n",
      "loss: 2.531585693359375\n",
      "loss: 2.52923846244812\n",
      "loss: 2.5269246101379395\n",
      "loss: 2.5246458053588867\n",
      "loss: 2.522399425506592\n",
      "loss: 2.520186185836792\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27*27).float() # input to the network: one-hot encoding\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # W -> zero, the more smooth distribution you're going achieve, \n",
    "    # 0.01 * (W**2).mean() called regularization\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "    print(f'loss: {loss.item()}')\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f871708-6864-4675-b580-a79065f7e73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexzdfzjglkuriana\n",
      "kayhhmvlzimjtna\n",
      "nalkfdkzka\n",
      "da\n",
      "samiyaubjtbhrigotwxezgzjeqkgxojkwptedo\n",
      "kaley\n",
      "masidey\n",
      "nkgvnrnfrftbspmhwcjdenvtahlvsuznsdrxdlngil\n",
      "pynw\n",
      "istnj\n",
      "ra\n",
      "danne\n",
      "zktsder\n",
      "jair\n",
      "t\n",
      "gbckajbhzabsvoth\n",
      "khysxqevecmpjxhcayhrieen\n",
      "xmvpfoqzmtrfvjbsdblmysox\n",
      "laptjapxzqbgpqlhariyannk\n",
      "ille\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    out = []\n",
    "    ix1 = 0 # .\n",
    "    ix2 = 0 # .\n",
    "    while True:\n",
    "        # ---------- CHANGE START ----------\n",
    "        \n",
    "        # 1. Turn the integer index 'ix' into a Tensor\n",
    "        x = torch.tensor([ix1 * 27 + ix2])\n",
    "        \n",
    "        # 2. One-hot encode the input (creates shape 1x(27*27))\n",
    "        # We must cast to .float() because W is float, but one_hot creates integers\n",
    "        xenc = F.one_hot(x, num_classes=27*27).float()\n",
    "        \n",
    "        # 3. Forward Pass: Calculate Logits\n",
    "        logits = xenc @ W \n",
    "        \n",
    "        # 4. Softmax: Convert Logits to Probabilities\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        # ---------- CHANGE END ----------\n",
    "        \n",
    "        # 5. Sample from the distribution (Same as before)\n",
    "        ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix3 == 0:\n",
    "            break\n",
    "        out.append(itos[ix3])\n",
    "        ix1 = ix2\n",
    "        ix2 = ix3\n",
    "    \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05bb369b-d275-41a2-b964-cac7935ec15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexzdfzjglkuriana\n",
      "kayhhmvlzimjtna\n",
      "nalkfdkzka\n",
      "da\n",
      "samiyaubjtbhrigotwxezgzjeqkgxojkwptedo\n",
      "kaley\n",
      "masidey\n",
      "nkgvnrnfrftbspmhwcjdenvtahlvsuznsdrxdlngil\n",
      "pynw\n",
      "istnj\n",
      "ra\n",
      "danne\n",
      "zktsder\n",
      "jair\n",
      "t\n",
      "gbckajbhzabsvoth\n",
      "khysxqevecmpjxhcayhrieen\n",
      "xmvpfoqzmtrfvjbsdblmysox\n",
      "laptjapxzqbgpqlhariyannk\n",
      "ille\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    out = []\n",
    "    ix1 = 0 # .\n",
    "    ix2 = 0 # .\n",
    "    while True:\n",
    "        # ---------- CHANGE START ----------\n",
    "        \n",
    "        # 1. Turn the integer index 'ix' into a Tensor\n",
    "        # x = torch.tensor([ix1 * 27 + ix2])\n",
    "        \n",
    "        # 2. One-hot encode the input (creates shape 1x(27*27))\n",
    "        # We must cast to .float() because W is float, but one_hot creates integers\n",
    "        # xenc = F.one_hot(x, num_classes=27*27).float()\n",
    "        \n",
    "        # 3. Forward Pass: Calculate Logits\n",
    "        # logits = xenc @ W \n",
    "\n",
    "\n",
    "        # OPTIMIZATION\n",
    "        # Multiply a one-hot vector by a matrix is exactly the same as plucking out a single\n",
    "        # row from that matrix\n",
    "\n",
    "        logits = W[ix1 * 27 + ix2]\n",
    "        \n",
    "        # 4. Softmax: Convert Logits to Probabilities\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum()\n",
    "        \n",
    "        # ---------- CHANGE END ----------\n",
    "        \n",
    "        # 5. Sample from the distribution (Same as before)\n",
    "        ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix3 == 0:\n",
    "            break\n",
    "        out.append(itos[ix3])\n",
    "        ix1 = ix2\n",
    "        ix2 = ix3\n",
    "    \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95d707be-c7e3-450b-9952-6996694387c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. \n",
    "# Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d90c95ee-ffa2-4c9d-941c-496f3bab406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25626 3203 3204\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# 1. Load\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "# 2. Shuffle (Crucial! Otherwise you might train on A->S and test on T-Z)\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "# 3. Calculate split points\n",
    "n1 = int(0.8 * len(words)) # 80% mark\n",
    "n2 = int(0.9 * len(words)) # 80% mark\n",
    "\n",
    "# 4. Slice\n",
    "words_train = words[:n1]  # 80%\n",
    "words_dev = words[n1:n2] # 80%-90%\n",
    "words_test = words[n2:]  # 90%-100%\n",
    "\n",
    "print(len(words_train), len(words_dev), len(words_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ce61bd4-f644-43a1-a112-7cd68d049d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Train...\n",
      "Dataset has 182625 examples\n",
      "Building Dev...\n",
      "Dataset has 22655 examples\n",
      "Building Test...\n",
      "Dataset has 22866 examples\n"
     ]
    }
   ],
   "source": [
    "# Mappings\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "def build_dataset(words):\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "        # Trigram context: start with two dots\n",
    "        chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            ix3 = stoi[ch3]\n",
    "            \n",
    "            # Trigram logic: (27 * ch1) + ch2\n",
    "            xs.append(ix1 * 27 + ix2)\n",
    "            ys.append(ix3)\n",
    "    \n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    print(f'Dataset has {xs.nelement()} examples')\n",
    "    return xs, ys\n",
    "\n",
    "# Build the three sets\n",
    "print(\"Building Train...\")\n",
    "Tri_Xtr, Tri_Ytr = build_dataset(words_train)\n",
    "print(\"Building Dev...\")\n",
    "Tri_Xdev, Tri_Ydev = build_dataset(words_dev)\n",
    "print(\"Building Test...\")\n",
    "Tri_Xte, Tri_Yte = build_dataset(words_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574580e-3be6-4b28-9f97-5e879a89e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "for alpha in np.arange(0.1, 0.5, 0.1):\n",
    "    \n",
    "    # Initialize network (Trigram: 729 inputs -> 27 outputs)\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    W = torch.randn((27*27, 27), generator=g, requires_grad=True)\n",
    "    \n",
    "    # --- TRAINING (only on Xtr, Ytr) ---\n",
    "    for k in range(100): # Increase this to ~1000 or more for good results\n",
    "        # Forward pass\n",
    "        xenc = F.one_hot(Tri_Xtr, num_classes=27*27).float()\n",
    "        logits = xenc @ W\n",
    "        \n",
    "        # Softmax\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        # Loss\n",
    "        # loss = -probs[torch.arange(len(Ytr)), Ytr].log().mean()\n",
    "        loss = -probs[torch.arange(len(Tri_Ytr)), Tri_Ytr].log().mean() + alpha * (W**2).mean()\n",
    "        \n",
    "        # Backward\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        W.data += -50 * W.grad\n",
    "    \n",
    "    print(f\"Training Loss: {loss.item()}\")\n",
    "    # --- EVALUATION (on Xdev, Ydev) ---\n",
    "    # We do NOT optimize W here, we just calculate loss\n",
    "    with torch.no_grad(): # Tells torch we don't need gradients here\n",
    "        xenc_dev = F.one_hot(Tri_Xdev, num_classes=27*27).float()\n",
    "        logits_dev = xenc_dev @ W\n",
    "        counts_dev = logits_dev.exp()\n",
    "        probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
    "        loss_dev = -probs_dev[torch.arange(len(Tri_Ydev)), Tri_Ydev].log().mean() + alpha * (W**2).mean()\n",
    "    \n",
    "    print(f\"Dev Loss: {loss_dev.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "870465cb-6838-44a5-a5c7-4a750e8e1a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Loss: 2.5264275074005127\n"
     ]
    }
   ],
   "source": [
    "# --- EVALUATION (on Xdev, Ydev) ---\n",
    "# We do NOT optimize W here, we just calculate loss\n",
    "with torch.no_grad(): # Tells torch we don't need gradients here\n",
    "    xenc_dev = F.one_hot(Tri_Xdev, num_classes=27*27).float()\n",
    "    logits_dev = xenc_dev @ W\n",
    "    counts_dev = logits_dev.exp()\n",
    "    probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
    "    loss_dev = -probs_dev[torch.arange(len(Tri_Ydev)), Tri_Ydev].log().mean() + 0.01 * (W**2).mean()\n",
    "\n",
    "print(f\"Dev Loss: {loss_dev.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ced0746-5d63-4fdb-bab9-05ea123131c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cce8d540-4708-4e84-abb5-ad6b11b00fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Train...\n",
      "Dataset has 182625 examples\n",
      "Building Dev...\n",
      "Dataset has 22655 examples\n",
      "Building Test...\n",
      "Dataset has 22866 examples\n"
     ]
    }
   ],
   "source": [
    "# Mappings\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "def build_dataset(words):\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "        # Bigram context: start with one dots\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            xs.append(ix1)\n",
    "            ys.append(ix2)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    \n",
    "    print(f'Dataset has {xs.nelement()} examples')\n",
    "    return xs, ys\n",
    "\n",
    "# Build the three sets\n",
    "print(\"Building Train...\")\n",
    "Bi_Xtr, Bi_Ytr = build_dataset(words_train)\n",
    "print(\"Building Dev...\")\n",
    "Bi_Xdev, Bi_Ydev = build_dataset(words_dev)\n",
    "print(\"Building Test...\")\n",
    "Bi_Xte, Bi_Yte = build_dataset(words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb239258-8752-41f2-a5a1-78a3ddaf66d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.587466239929199\n",
      "Dev Loss: 2.5862185955047607\n",
      "Training Loss: 2.654571771621704\n",
      "Dev Loss: 2.653724431991577\n",
      "Training Loss: 2.7046115398406982\n",
      "Dev Loss: 2.704026460647583\n",
      "Training Loss: 2.744701862335205\n",
      "Dev Loss: 2.7443368434906006\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "for alpha in np.arange(0.1, 0.5, 0.1):\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "    \n",
    "    # --- TRAINING (only on Xtr, Ytr) ---\n",
    "    for k in range(100): # Increase this to ~1000 or more for good results\n",
    "        # Forward pass\n",
    "        xenc = F.one_hot(Bi_Xtr, num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        \n",
    "        # Softmax\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        # Loss\n",
    "        # loss = -probs[torch.arange(len(Ytr)), Ytr].log().mean()\n",
    "        loss = -probs[torch.arange(len(Bi_Xtr)), Bi_Ytr].log().mean() + alpha * (W**2).mean()\n",
    "        \n",
    "        # Backward\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        W.data += -50 * W.grad\n",
    "    \n",
    "    print(f\"Training Loss: {loss.item()}\")\n",
    "    # --- EVALUATION (on Xdev, Ydev) ---\n",
    "    # We do NOT optimize W here, we just calculate loss\n",
    "    with torch.no_grad(): # Tells torch we don't need gradients here\n",
    "        xenc_dev = F.one_hot(Bi_Xdev, num_classes=27).float()\n",
    "        logits_dev = xenc_dev @ W\n",
    "        counts_dev = logits_dev.exp()\n",
    "        probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
    "        loss_dev = -probs_dev[torch.arange(len(Bi_Ydev)), Bi_Ydev].log().mean() + alpha * (W**2).mean()\n",
    "    \n",
    "    print(f\"Dev Loss: {loss_dev.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07090653-bf3e-467c-80d5-8349df266c58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Torch)",
   "language": "python",
   "name": "py39_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
