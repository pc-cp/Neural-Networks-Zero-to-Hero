{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8af4883e-bd6c-4ade-839d-7a65fecf090f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c7283-24be-47a2-b638-a84df233a3c8",
   "metadata": {},
   "source": [
    "# E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0dd9a197-ab24-4b3b-b600-16c8c863a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# creates the training set of trigrams (x, y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    # Trigram context requires TWO starting characters\n",
    "    # If word is 'emma', context is:\n",
    "    # .. -> e\n",
    "    # .e -> m\n",
    "    # em -> m\n",
    "    # mm -> a\n",
    "    # ma -> .\n",
    "    chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3] # target\n",
    "        # Create a unique idx for the input pair (ch1, ch2)\n",
    "        # this maps (0, 0) -> 0, (0, 1) -> 1 ... (26, 26) -> 728\n",
    "        trigram_idx = ix1 * 27 + ix2 # base-27\n",
    "        xs.append(trigram_idx)\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "xs = xs.to(device)\n",
    "ys = ys.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3cafcb58-4064-4d42-b6b3-9aa36289f2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0, loss: 3.762385368347168\n",
      "idx: 10, loss: 3.2260546684265137\n",
      "idx: 20, loss: 2.9909610748291016\n",
      "idx: 30, loss: 2.8569254875183105\n",
      "idx: 40, loss: 2.7668895721435547\n",
      "idx: 50, loss: 2.7005512714385986\n",
      "idx: 60, loss: 2.6489357948303223\n",
      "idx: 70, loss: 2.607341766357422\n",
      "idx: 80, loss: 2.5730233192443848\n",
      "idx: 90, loss: 2.5442166328430176\n"
     ]
    }
   ],
   "source": [
    "# initialize the 'network'\n",
    "g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g, requires_grad=True, device=device)\n",
    "# gradient descent\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27*27).float() # input to the network: one-hot encoding\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # W -> zero, the more smooth distribution you're going achieve, \n",
    "    # 0.01 * (W**2).mean() called regularization\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "    if k % 10 == 0:\n",
    "        print(f'idx: {k}, loss: {loss.item()}')\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8f871708-6864-4675-b580-a79065f7e73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "khed\n",
      "epwylingyyzlbgomi\n",
      "ana\n",
      "maqkscyamicxobriafxine\n",
      "delenlzdu\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix1 = 0 # .\n",
    "    ix2 = 0 # .\n",
    "    while True:\n",
    "        # ---------- CHANGE START ----------\n",
    "        \n",
    "        # 1. Turn the integer index 'ix' into a Tensor\n",
    "        x = torch.tensor([ix1 * 27 + ix2])\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # 2. One-hot encode the input (creates shape 1x(27*27))\n",
    "        # We must cast to .float() because W is float, but one_hot creates integers\n",
    "        xenc = F.one_hot(x, num_classes=27*27).float()\n",
    "        \n",
    "        # 3. Forward Pass: Calculate Logits\n",
    "        logits = xenc @ W \n",
    "        \n",
    "        # 4. Softmax: Convert Logits to Probabilities\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        # ---------- CHANGE END ----------\n",
    "        \n",
    "        # 5. Sample from the distribution (Same as before)\n",
    "        ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix3 == 0:\n",
    "            break\n",
    "        out.append(itos[ix3])\n",
    "        ix1 = ix2\n",
    "        ix2 = ix3\n",
    "    \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05bb369b-d275-41a2-b964-cac7935ec15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "khed\n",
      "epwylingyyzlbgomi\n",
      "ana\n",
      "maqkscyamicxobriafxine\n",
      "delenlzdu\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix1 = 0 # .\n",
    "    ix2 = 0 # .\n",
    "    while True:\n",
    "        # ---------- CHANGE START ----------\n",
    "        \n",
    "        # 1. Turn the integer index 'ix' into a Tensor\n",
    "        # x = torch.tensor([ix1 * 27 + ix2])\n",
    "        \n",
    "        # 2. One-hot encode the input (creates shape 1x(27*27))\n",
    "        # We must cast to .float() because W is float, but one_hot creates integers\n",
    "        # xenc = F.one_hot(x, num_classes=27*27).float()\n",
    "        \n",
    "        # 3. Forward Pass: Calculate Logits\n",
    "        # logits = xenc @ W \n",
    "\n",
    "\n",
    "        # OPTIMIZATION\n",
    "        # Multiply a one-hot vector by a matrix is exactly the same as plucking out a single\n",
    "        # row from that matrix\n",
    "\n",
    "        logits = W[ix1 * 27 + ix2]\n",
    "        \n",
    "        # 4. Softmax: Convert Logits to Probabilities\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum()\n",
    "        \n",
    "        # ---------- CHANGE END ----------\n",
    "        \n",
    "        # 5. Sample from the distribution (Same as before)\n",
    "        ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix3 == 0:\n",
    "            break\n",
    "        out.append(itos[ix3])\n",
    "        ix1 = ix2\n",
    "        ix2 = ix3\n",
    "    \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63749b-9609-4521-8e2d-78b291ef2633",
   "metadata": {},
   "source": [
    "|Epoches|Bigram loss|Trigram loss|\n",
    "|:-----:|-----------|------------|\n",
    "|50   |   2.49662 |**2.6964**|\n",
    "|100  |   2.47    |2.51169|\n",
    "|200  |   2.461897|2.37919|\n",
    "|2000 |   **2.454**   |**2.2116**|\n",
    "\n",
    "1. when epoches very small, like 50 or 100. loss of Trigram large Bigram. so complex network needs more epoches to feed neurons. for large epoches, complex network have lower loss. means its complex allow it fit data greatly.\n",
    "2. when epoch increase, Bigram loss seems decrease **slowly**. But Trigram loss seems decreases quickly. so Trigram's capacity allow its learn more information from 2-order data but Bigram seems its limited network prevent its learning.\n",
    "3. if data limited, use shallow network instead of complex network, otherwise not feed it up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d707be-c7e3-450b-9952-6996694387c5",
   "metadata": {},
   "source": [
    "# E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d90c95ee-ffa2-4c9d-941c-496f3bab406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6406 3203 3204\n"
     ]
    }
   ],
   "source": [
    "# 1. Shuffle (Crucial! Otherwise you might train on A->S and test on T-Z)\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "# 2. Calculate split points\n",
    "n_e03 = int(0.2 * len(words)) # 80% mark\n",
    "n1 = int(0.8 * len(words)) # 80% mark\n",
    "n2 = int(0.9 * len(words)) # 80% mark\n",
    "\n",
    "# 3. Slice\n",
    "words_train = words[:n_e03]  # 80%\n",
    "words_dev = words[n1:n2] # 80%-90%\n",
    "words_test = words[n2:]  # 90%-100%\n",
    "\n",
    "print(len(words_train), len(words_dev), len(words_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7ce61bd4-f644-43a1-a112-7cd68d049d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Train...\n",
      "Dataset has 45693 examples\n",
      "Building Dev...\n",
      "Dataset has 22767 examples\n",
      "Building Test...\n",
      "Dataset has 22799 examples\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "        # Trigram context: start with two dots\n",
    "        chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            ix3 = stoi[ch3]\n",
    "            \n",
    "            # Trigram logic: (27 * ch1) + ch2\n",
    "            xs.append(ix1 * 27 + ix2)\n",
    "            ys.append(ix3)\n",
    "    \n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    xs = xs.to(device)\n",
    "    ys = ys.to(device)\n",
    "    print(f'Dataset has {xs.nelement()} examples')\n",
    "    return xs, ys\n",
    "\n",
    "# Build the three sets\n",
    "print(\"Building Train...\")\n",
    "Tri_Xtr, Tri_Ytr = build_dataset(words_train)\n",
    "print(\"Building Dev...\")\n",
    "Tri_Xdev, Tri_Ydev = build_dataset(words_dev)\n",
    "print(\"Building Test...\")\n",
    "Tri_Xte, Tri_Yte = build_dataset(words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3574580e-3be6-4b28-9f97-5e879a89e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha: 0.0\n",
      "Training Loss: 2.6972665786743164\n",
      "Dev Loss: 2.6845288276672363\n",
      "Test Loss: 2.698153018951416\n",
      "\n",
      "Alpha: 0.05\n",
      "Training Loss: 2.7422003746032715\n",
      "Dev Loss: 2.6827549934387207\n",
      "Test Loss: 2.696232795715332\n"
     ]
    }
   ],
   "source": [
    "for alpha in np.arange(0, 0.1, 0.05):\n",
    "    print(f\"\\nAlpha: {alpha}\")\n",
    "    # Initialize network (Trigram: 729 inputs -> 27 outputs)\n",
    "    g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "    W = torch.randn((27*27, 27), generator=g, requires_grad=True, device=device)\n",
    "    \n",
    "    # --- TRAINING (only on Xtr, Ytr) ---\n",
    "    for k in range(50): # Increase this to ~1000 or more for good results\n",
    "        # Forward pass\n",
    "        xenc = F.one_hot(Tri_Xtr, num_classes=27*27).float()\n",
    "        logits = xenc @ W\n",
    "        \n",
    "        # Softmax\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        # Loss\n",
    "        loss = -probs[torch.arange(len(Tri_Ytr)), Tri_Ytr].log().mean() + alpha * (W**2).mean()\n",
    "        \n",
    "        # Backward\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        W.data += -50 * W.grad\n",
    "    \n",
    "    print(f\"Training Loss: {loss.item()}\")\n",
    "    # --- EVALUATION (on Xdev, Ydev) ---\n",
    "    # We do NOT optimize W here, we just calculate loss\n",
    "    with torch.no_grad(): # Tells torch we don't need gradients here\n",
    "        xenc_dev = F.one_hot(Tri_Xdev, num_classes=27*27).float()\n",
    "        logits_dev = xenc_dev @ W\n",
    "        counts_dev = logits_dev.exp()\n",
    "        probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
    "        loss_dev = -probs_dev[torch.arange(len(Tri_Ydev)), Tri_Ydev].log().mean()\n",
    "\n",
    "    print(f\"Dev Loss: {loss_dev.item()}\")\n",
    "\n",
    "    # --- TEST (on Xte, Yte) ---\n",
    "    with torch.no_grad():\n",
    "        xenc_test = F.one_hot(Tri_Xte, num_classes=27*27).float()\n",
    "        logits_test = xenc_test @ W\n",
    "        counts_test = logits_test.exp()\n",
    "        probs_test = counts_test / counts_test.sum(1, keepdim =True)\n",
    "        loss_test = -probs_test[torch.arange(len(Tri_Yte)), Tri_Yte].log().mean()\n",
    "        # Usually we DO NOT include regularization in test/dev loss\n",
    "        # because regularization is a training penalty,\n",
    "        # not part of the true model performance.\n",
    "    print(f\"Test Loss: {loss_test.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cce8d540-4708-4e84-abb5-ad6b11b00fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Train...\n",
      "Dataset has 182625 examples\n",
      "Building Dev...\n",
      "Dataset has 22655 examples\n",
      "Building Test...\n",
      "Dataset has 22866 examples\n"
     ]
    }
   ],
   "source": [
    "# Bigram \n",
    "def build_dataset(words):\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "        # Bigram context: start with one dots\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            xs.append(ix1)\n",
    "            ys.append(ix2)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    xs = xs.to(device)\n",
    "    ys = ys.to(device)\n",
    "    print(f'Dataset has {xs.nelement()} examples')\n",
    "    return xs, ys\n",
    "\n",
    "# Build the three sets\n",
    "print(\"Building Train...\")\n",
    "Bi_Xtr, Bi_Ytr = build_dataset(words_train)\n",
    "print(\"Building Dev...\")\n",
    "Bi_Xdev, Bi_Ydev = build_dataset(words_dev)\n",
    "print(\"Building Test...\")\n",
    "Bi_Xte, Bi_Yte = build_dataset(words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cb239258-8752-41f2-a5a1-78a3ddaf66d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha: 0.0\n",
      "Training Loss: 2.496650457382202\n",
      "Dev Loss: 2.492772340774536\n",
      "Test Loss: 2.500807285308838\n",
      "\n",
      "Alpha: 0.05\n",
      "Training Loss: 2.5556743144989014\n",
      "Dev Loss: 2.501662492752075\n",
      "Test Loss: 2.508352518081665\n"
     ]
    }
   ],
   "source": [
    "for alpha in np.arange(0, 0.1, 0.05):\n",
    "    print(f\"\\nAlpha: {alpha}\")\n",
    "    g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "    W = torch.randn((27, 27), generator=g, requires_grad=True, device=device)\n",
    "    \n",
    "    # --- TRAINING (only on Xtr, Ytr) ---\n",
    "    for k in range(50): # Increase this to ~1000 or more for good results\n",
    "        # Forward pass\n",
    "        xenc = F.one_hot(Bi_Xtr, num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        \n",
    "        # Softmax\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        # Loss\n",
    "        loss = -probs[torch.arange(len(Bi_Xtr)), Bi_Ytr].log().mean() + alpha * (W**2).mean()\n",
    "        \n",
    "        # Backward\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        W.data += -50 * W.grad\n",
    "    \n",
    "    print(f\"Training Loss: {loss.item()}\")\n",
    "    # --- EVALUATION (on Xdev, Ydev) ---\n",
    "    # We do NOT optimize W here, we just calculate loss\n",
    "    with torch.no_grad(): # Tells torch we don't need gradients here\n",
    "        xenc_dev = F.one_hot(Bi_Xdev, num_classes=27).float()\n",
    "        logits_dev = xenc_dev @ W\n",
    "        counts_dev = logits_dev.exp()\n",
    "        probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
    "        loss_dev = -probs_dev[torch.arange(len(Bi_Ydev)), Bi_Ydev].log().mean()\n",
    "    \n",
    "    print(f\"Dev Loss: {loss_dev.item()}\")\n",
    "\n",
    "    # --- TEST (on Xte, Yte) ---\n",
    "    with torch.no_grad():\n",
    "        xenc_test = F.one_hot(Bi_Xte, num_classes=27).float()\n",
    "        logits_test = xenc_test @ W\n",
    "        counts_test = logits_test.exp()\n",
    "        probs_test = counts_test / counts_test.sum(1, keepdims=True)\n",
    "        loss_test = -probs_test[torch.arange(len(Bi_Yte)), Bi_Yte].log().mean()\n",
    "        # Usually we DO NOT include regularization in test/dev loss\n",
    "        # because regularization is a training penalty,\n",
    "        # not part of the true model performance.\n",
    "    print(f\"Test Loss: {loss_test.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07090653-bf3e-467c-80d5-8349df266c58",
   "metadata": {},
   "source": [
    "epoch = 2000\n",
    "## Trigram\n",
    "1. Alpha: 0.0\n",
    "2. Training Loss: 2.2116944789886475\n",
    "3. Dev Loss: 2.2344369888305664\n",
    "4. Test Loss: 2.235217571258545\n",
    "## Bigram\n",
    "1. Alpha: 0.0\n",
    "2. Training Loss: 2.45473575592041\n",
    "3. Dev Loss: 2.4530274868011475\n",
    "4. Test Loss: 2.458131790161133\n",
    "\n",
    "1. 可以发现Trigram loss更低，说明其参数容量允许其进一步收敛，建模更多统计数据。但dev和test loss > train loss，说明？[模型开始记住数据中的噪声，过拟合]\n",
    "2. Bigram loss较高，其参数容量无法表达更复杂的结构。同时三个loss之间都比较接近。说明？[几乎没有过拟合，其模型容量不允许其overfit，尽管epoch很大]\n",
    "3. train and dev loss relationship:\n",
    "   1. train << dev -> overfit\n",
    "   2. train almost = dev -> underfit or capacity of network limited\n",
    "4. loss between trigram and bigram: loss trigram lower -> 2-order context provide more extra information.\n",
    "5. other problems?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74937ad8-b588-4062-b74c-03f1a01ff767",
   "metadata": {},
   "source": [
    "# E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0a0853e7-5c05-433c-b428-d12f95062414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha: 0.0\n",
      "Training Loss: 2.247952461242676\n",
      "Dev Loss: 2.3174901008605957\n",
      "\n",
      "Alpha: 0.005\n",
      "Training Loss: 2.2541582584381104\n",
      "Dev Loss: 2.31742525100708\n",
      "\n",
      "Alpha: 0.01\n",
      "Training Loss: 2.2601585388183594\n",
      "Dev Loss: 2.317377805709839\n",
      "\n",
      "Alpha: 0.015\n",
      "Training Loss: 2.2659599781036377\n",
      "Dev Loss: 2.317347764968872\n",
      "\n",
      "Alpha: 0.02\n",
      "Training Loss: 2.2715699672698975\n",
      "Dev Loss: 2.3173348903656006\n",
      "\n",
      "Alpha: 0.025\n",
      "Training Loss: 2.276995897293091\n",
      "Dev Loss: 2.317338705062866\n",
      "\n",
      "Alpha: 0.03\n",
      "Training Loss: 2.282243490219116\n",
      "Dev Loss: 2.3173584938049316\n",
      "\n",
      "Alpha: 0.035\n",
      "Training Loss: 2.287320137023926\n",
      "Dev Loss: 2.317394256591797\n",
      "\n",
      "Alpha: 0.04\n",
      "Training Loss: 2.292231798171997\n",
      "Dev Loss: 2.3174455165863037\n",
      "\n",
      "Alpha: 0.045\n",
      "Training Loss: 2.2969841957092285\n",
      "Dev Loss: 2.317512035369873\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "alphas = []\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "\n",
    "for alpha in np.arange(0, 0.05, 0.005):\n",
    "    alphas.append(alpha)\n",
    "    print(f\"\\nAlpha: {alpha}\")\n",
    "    # Initialize network (Trigram: 729 inputs -> 27 outputs)\n",
    "    g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "    W = torch.randn((27*27, 27), generator=g, requires_grad=True, device=device)\n",
    "    \n",
    "    # --- TRAINING (only on Xtr, Ytr) ---\n",
    "    for k in range(500): # Increase this to ~1000 or more for good results\n",
    "        # Forward pass\n",
    "        xenc = F.one_hot(Tri_Xtr, num_classes=27*27).float()\n",
    "        logits = xenc @ W\n",
    "        \n",
    "        # Softmax\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        # Loss\n",
    "        loss = -probs[torch.arange(len(Tri_Ytr)), Tri_Ytr].log().mean() + alpha * (W**2).mean()\n",
    "        \n",
    "        # Backward\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        W.data += -50 * W.grad\n",
    "    \n",
    "    print(f\"Training Loss: {loss.item()}\")\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # --- EVALUATION (on Xdev, Ydev) ---\n",
    "    # We do NOT optimize W here, we just calculate loss\n",
    "    with torch.no_grad(): # Tells torch we don't need gradients here\n",
    "        xenc_dev = F.one_hot(Tri_Xdev, num_classes=27*27).float()\n",
    "        logits_dev = xenc_dev @ W\n",
    "        counts_dev = logits_dev.exp()\n",
    "        probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
    "        loss_dev = -probs_dev[torch.arange(len(Tri_Ydev)), Tri_Ydev].log().mean()\n",
    "\n",
    "    print(f\"Dev Loss: {loss_dev.item()}\")\n",
    "    dev_losses.append(loss_dev.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "36b08033-5940-406b-a388-8d8e8bc3df67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(alphas, train_losses)\n",
    "plt.plot(alphas, dev_losses)\n",
    "\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Alpha vs Train and Dev Loss (Trigram)\")\n",
    "plt.legend([\"Train\", \"Dev\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b3b7064-78d4-4eea-a268-948925d5ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha: 0.05\n",
      "Training Loss: 2.3015832901000977\n",
      "Dev Loss: 2.3175933361053467\n",
      "\n",
      "Alpha: 0.055\n",
      "Training Loss: 2.306034803390503\n",
      "Dev Loss: 2.3176889419555664\n",
      "\n",
      "Alpha: 0.06\n",
      "Training Loss: 2.3103435039520264\n",
      "Dev Loss: 2.3177988529205322\n",
      "\n",
      "Alpha: 0.065\n",
      "Training Loss: 2.3145158290863037\n",
      "Dev Loss: 2.317922830581665\n",
      "\n",
      "Alpha: 0.06999999999999999\n",
      "Training Loss: 2.318556308746338\n",
      "Dev Loss: 2.3180599212646484\n",
      "\n",
      "Alpha: 0.07499999999999998\n",
      "Training Loss: 2.3224692344665527\n",
      "Dev Loss: 2.3182103633880615\n",
      "\n",
      "Alpha: 0.07999999999999999\n",
      "Training Loss: 2.3262598514556885\n",
      "Dev Loss: 2.318373680114746\n",
      "\n",
      "Alpha: 0.08499999999999999\n",
      "Training Loss: 2.3299331665039062\n",
      "Dev Loss: 2.318549633026123\n",
      "\n",
      "Alpha: 0.08999999999999998\n",
      "Training Loss: 2.3334927558898926\n",
      "Dev Loss: 2.318737745285034\n",
      "\n",
      "Alpha: 0.09499999999999997\n",
      "Training Loss: 2.336942672729492\n",
      "Dev Loss: 2.3189377784729004\n"
     ]
    }
   ],
   "source": [
    "for alpha in np.arange(0.05, 0.1, 0.005):\n",
    "    alphas.append(alpha)\n",
    "    print(f\"\\nAlpha: {alpha}\")\n",
    "    # Initialize network (Trigram: 729 inputs -> 27 outputs)\n",
    "    g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "    W = torch.randn((27*27, 27), generator=g, requires_grad=True, device=device)\n",
    "    \n",
    "    # --- TRAINING (only on Xtr, Ytr) ---\n",
    "    for k in range(500): # Increase this to ~1000 or more for good results\n",
    "        # Forward pass\n",
    "        xenc = F.one_hot(Tri_Xtr, num_classes=27*27).float()\n",
    "        logits = xenc @ W\n",
    "        \n",
    "        # Softmax\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        # Loss\n",
    "        loss = -probs[torch.arange(len(Tri_Ytr)), Tri_Ytr].log().mean() + alpha * (W**2).mean()\n",
    "        \n",
    "        # Backward\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        W.data += -50 * W.grad\n",
    "    \n",
    "    print(f\"Training Loss: {loss.item()}\")\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # --- EVALUATION (on Xdev, Ydev) ---\n",
    "    # We do NOT optimize W here, we just calculate loss\n",
    "    with torch.no_grad(): # Tells torch we don't need gradients here\n",
    "        xenc_dev = F.one_hot(Tri_Xdev, num_classes=27*27).float()\n",
    "        logits_dev = xenc_dev @ W\n",
    "        counts_dev = logits_dev.exp()\n",
    "        probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
    "        loss_dev = -probs_dev[torch.arange(len(Tri_Ydev)), Tri_Ydev].log().mean()\n",
    "\n",
    "    print(f\"Dev Loss: {loss_dev.item()}\")\n",
    "    dev_losses.append(loss_dev.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "505589e7-ff05-4795-8212-1c3318bf89aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05, 0.055, 0.06, 0.065, 0.06999999999999999, 0.07499999999999998, 0.07999999999999999, 0.08499999999999999, 0.08999999999999998, 0.09499999999999997]\n",
      "[2.247952461242676, 2.2541582584381104, 2.2601585388183594, 2.2659599781036377, 2.2715699672698975, 2.276995897293091, 2.282243490219116, 2.287320137023926, 2.292231798171997, 2.2969841957092285, 2.3015832901000977, 2.306034803390503, 2.3103435039520264, 2.3145158290863037, 2.318556308746338, 2.3224692344665527, 2.3262598514556885, 2.3299331665039062, 2.3334927558898926, 2.336942672729492]\n",
      "[2.3174901008605957, 2.31742525100708, 2.317377805709839, 2.317347764968872, 2.3173348903656006, 2.317338705062866, 2.3173584938049316, 2.317394256591797, 2.3174455165863037, 2.317512035369873, 2.3175933361053467, 2.3176889419555664, 2.3177988529205322, 2.317922830581665, 2.3180599212646484, 2.3182103633880615, 2.318373680114746, 2.318549633026123, 2.318737745285034, 2.3189377784729004]\n",
      "Best index: 4\n",
      "Best alpha: 0.02\n",
      "Best dev loss: 2.3173348903656006\n"
     ]
    }
   ],
   "source": [
    "print(alphas)\n",
    "print(train_losses)\n",
    "print(dev_losses)\n",
    "dev_losses = np.array(dev_losses)\n",
    "\n",
    "best_index = np.argmin(dev_losses)\n",
    "best_alpha = alphas[best_index]\n",
    "best_dev_loss = dev_losses[best_index]\n",
    "\n",
    "print(\"Best index:\", best_index)\n",
    "print(\"Best alpha:\", best_alpha)\n",
    "print(\"Best dev loss:\", best_dev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a2ef3f76-dcc5-45d0-b51b-450dabb145cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha: 0.02\n",
      "Training Loss: 2.2715699672698975\n",
      "Dev Loss: 2.3173348903656006\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.02\n",
    "print(f\"\\nAlpha: {alpha}\")\n",
    "# Initialize network (Trigram: 729 inputs -> 27 outputs)\n",
    "g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g, requires_grad=True, device=device)\n",
    "\n",
    "# --- TRAINING (only on Xtr, Ytr) ---\n",
    "for k in range(500): # Increase this to ~1000 or more for good results\n",
    "    # Forward pass\n",
    "    xenc = F.one_hot(Tri_Xtr, num_classes=27*27).float()\n",
    "    logits = xenc @ W\n",
    "    \n",
    "    # Softmax\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    \n",
    "    # Loss\n",
    "    loss = -probs[torch.arange(len(Tri_Ytr)), Tri_Ytr].log().mean() + alpha * (W**2).mean()\n",
    "    \n",
    "    # Backward\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -50 * W.grad\n",
    "\n",
    "print(f\"Training Loss: {loss.item()}\")\n",
    "\n",
    "# --- EVALUATION (on Xdev, Ydev) ---\n",
    "# We do NOT optimize W here, we just calculate loss\n",
    "with torch.no_grad(): # Tells torch we don't need gradients here\n",
    "    xenc_dev = F.one_hot(Tri_Xdev, num_classes=27*27).float()\n",
    "    logits_dev = xenc_dev @ W\n",
    "    counts_dev = logits_dev.exp()\n",
    "    probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
    "    loss_dev = -probs_dev[torch.arange(len(Tri_Ydev)), Tri_Ydev].log().mean()\n",
    "\n",
    "print(f\"Dev Loss: {loss_dev.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dbf46608-0931-421e-8faa-47bac3a4ba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3165476322174072\n"
     ]
    }
   ],
   "source": [
    "# --- TEST (on Xte, Yte) ---\n",
    "with torch.no_grad():\n",
    "    xenc_test = F.one_hot(Tri_Xte, num_classes=27*27).float()\n",
    "    logits_test = xenc_test @ W\n",
    "    counts_test = logits_test.exp()\n",
    "    probs_test = counts_test / counts_test.sum(1, keepdim =True)\n",
    "    loss_test = -probs_test[torch.arange(len(Tri_Yte)), Tri_Yte].log().mean()\n",
    "    # Usually we DO NOT include regularization in test/dev loss\n",
    "    # because regularization is a training penalty,\n",
    "    # not part of the true model performance.\n",
    "print(f\"Test Loss: {loss_test.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a525ce98-58c0-48af-bdbe-60e07029547e",
   "metadata": {},
   "source": [
    "# chatgpt say:\n",
    "1. normally we can find dev loss first decrease because suitable alpha can control overfit. then dev loss will increase \\\n",
    "because large alpha will harm model learning -> **U** shape\n",
    "2. if dev loss **monotonously increase** means model still underfit or fit fine but not overfit. so small alpha will not influence model. but large alpha still harm it.\n",
    "3. so simulate overfit. we just use 20% dataset as train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac052a07-a61a-4828-b03c-47c40a50d410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
